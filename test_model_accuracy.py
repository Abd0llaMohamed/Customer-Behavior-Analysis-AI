import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("ğŸ“Š Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
print("="*80)

# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
try:
    df = pd.read_excel('customers_kaggle_improved.xlsx')
    print(f"\nâœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {len(df)} Ø¹Ù…ÙŠÙ„")
except FileNotFoundError:
    print("\nâŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: customers_kaggle.xlsx")
    print("ğŸ“¥ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ´ØºÙŠÙ„ ÙƒÙˆØ¯ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø£ÙˆÙ„Ø§Ù‹")
    input("\nØ§Ø¶ØºØ· Enter Ù„Ù„Ø®Ø±ÙˆØ¬...")
    exit()

# Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
feature_cols = ['Purchases', 'Total_Value', 'Visits']
X = df[feature_cols].values

print(f"\nğŸ“‹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {feature_cols}")
print(f"\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
print(df[feature_cols].describe())

# ========== Ø§Ø®ØªØ¨Ø§Ø± 1: Clustering (K-Means) ==========
print("\n" + "="*80)
print("ğŸ“ Ø§Ø®ØªØ¨Ø§Ø± 1: Clustering (K-Means)")
print("="*80)

# ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ØªØ¬Ø±Ø¨Ø© Ø£Ø¹Ø¯Ø§Ø¯ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ù€ clusters
print("\nğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¹Ø¯Ø§Ø¯ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ù€ clusters...")
results = []

for n_clusters in [2, 3, 4, 5]:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)

    # Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¬ÙˆØ¯Ø©
    silhouette = silhouette_score(X_scaled, labels)
    davies_bouldin = davies_bouldin_score(X_scaled, labels)
    calinski = calinski_harabasz_score(X_scaled, labels)

    results.append({
        'n_clusters': n_clusters,
        'silhouette': silhouette,
        'davies_bouldin': davies_bouldin,
        'calinski': calinski
    })

    print(f"\n  {n_clusters} Clusters:")
    print(f"    Silhouette Score: {silhouette:.4f} (Ø£Ø¹Ù„Ù‰ = Ø£ÙØ¶Ù„ØŒ Ø§Ù„Ù…Ø¯Ù‰: -1 Ø¥Ù„Ù‰ 1)")
    print(f"    Davies-Bouldin Index: {davies_bouldin:.4f} (Ø£Ù‚Ù„ = Ø£ÙØ¶Ù„)")
    print(f"    Calinski-Harabasz Score: {calinski:.2f} (Ø£Ø¹Ù„Ù‰ = Ø£ÙØ¶Ù„)")

# Ø§Ù„Ø£ÙØ¶Ù„
best_silhouette = max(results, key=lambda x: x['silhouette'])
print(f"\nâœ… Ø£ÙØ¶Ù„ Ø¹Ø¯Ø¯ clusters Ø­Ø³Ø¨ Silhouette: {best_silhouette['n_clusters']}")
print(f"   Silhouette Score: {best_silhouette['silhouette']:.4f}")

# ========== Ø§Ø®ØªØ¨Ø§Ø± 2: Classification (Churn Prediction) ==========
print("\n" + "="*80)
print("ğŸ“ Ø§Ø®ØªØ¨Ø§Ø± 2: Churn Prediction (Random Forest)")
print("="*80)

# Ø¥Ù†Ø´Ø§Ø¡ target Ù…ØªÙˆÙ‚Ø¹ (churn simulation)
# Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø¨Ù€ Purchases Ù‚Ù„ÙŠÙ„Ø© Ùˆ Total_Value Ù‚Ù„ÙŠÙ„Ø© = Ù…Ø­ØªÙ…Ù„ ÙŠØªØ±ÙƒÙˆØ§
df['churn'] = ((df['Purchases'] <= df['Purchases'].quantile(0.25)) & 
               (df['Total_Value'] <= df['Total_Value'].quantile(0.25))).astype(int)

print(f"\nğŸ“Š ØªÙˆØ²ÙŠØ¹ Churn:")
print(f"   Churned (1): {df['churn'].sum()} Ø¹Ù…ÙŠÙ„ ({df['churn'].sum()/len(df)*100:.1f}%)")
print(f"   Active (0): {(1-df['churn']).sum()} Ø¹Ù…ÙŠÙ„ ({(1-df['churn']).sum()/len(df)*100:.1f}%)")

# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, df['churn'], test_size=0.2, random_state=42, stratify=df['churn']
)

print(f"\nğŸ“Š Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
print(f"   Training: {len(X_train)} Ø¹Ù…ÙŠÙ„")
print(f"   Testing: {len(X_test)} Ø¹Ù…ÙŠÙ„")

# ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
print(f"\nğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ¯Ø±ÙŠØ¨ Random Forest...")
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)
rf_model.fit(X_train, y_train)

# Ø§Ù„ØªÙ†Ø¨Ø¤
y_pred = rf_model.predict(X_test)

# Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, zero_division=0)
recall = recall_score(y_test, y_pred, zero_division=0)
f1 = f1_score(y_test, y_pred, zero_division=0)

print(f"\nâœ… Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:")
print(f"   Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"   Precision: {precision:.4f} ({precision*100:.2f}%)")
print(f"   Recall: {recall:.4f} ({recall*100:.2f}%)")
print(f"   F1-Score: {f1:.4f} ({f1*100:.2f}%)")

# Cross-validation
print(f"\nğŸ”„ Ø¬Ø§Ø±ÙŠ Cross-Validation (5-fold)...")
cv_scores = cross_val_score(rf_model, X_scaled, df['churn'], cv=5, scoring='accuracy')
print(f"\nâœ… Cross-Validation Scores:")
print(f"   Scores: {[f'{s:.4f}' for s in cv_scores]}")
print(f"   Mean: {cv_scores.mean():.4f} ({cv_scores.mean()*100:.2f}%)")
print(f"   Std: {cv_scores.std():.4f}")

# Feature Importance
feature_importance = pd.DataFrame({
    'Feature': feature_cols,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

print(f"\nğŸ“Š Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø®ØµØ§Ø¦Øµ (Feature Importance):")
for idx, row in feature_importance.iterrows():
    print(f"   {row['Feature']}: {row['Importance']:.4f} ({row['Importance']*100:.1f}%)")

# ========== Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ==========
print("\n" + "="*80)
print("ğŸ“‹ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")
print("="*80)

print(f"\n1ï¸âƒ£ Clustering Quality:")
print(f"   âœ… Silhouette Score: {best_silhouette['silhouette']:.4f}")
if best_silhouette['silhouette'] > 0.5:
    print(f"   ğŸ‰ Ù…Ù…ØªØ§Ø² - Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØªØ¬Ù…Ø¹ Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­")
elif best_silhouette['silhouette'] > 0.3:
    print(f"   âœ… Ø¬ÙŠØ¯ - Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØªØ¬Ù…Ø¹ Ø¨Ø´ÙƒÙ„ Ù…Ø¹Ù‚ÙˆÙ„")
elif best_silhouette['silhouette'] > 0.2:
    print(f"   âš ï¸ Ù…ØªÙˆØ³Ø· - Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØªØ¬Ù…Ø¹ Ø¨Ø´ÙƒÙ„ Ø¶Ø¹ÙŠÙ")
else:
    print(f"   âŒ Ø¶Ø¹ÙŠÙ - Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø§ ØªØªØ¬Ù…Ø¹ Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­")

print(f"\n2ï¸âƒ£ Classification Accuracy:")
print(f"   âœ… Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"   âœ… CV Mean Accuracy: {cv_scores.mean():.4f} ({cv_scores.mean()*100:.2f}%)")
if accuracy > 0.85:
    print(f"   ğŸ‰ Ø¯Ù‚Ø© Ù…Ù…ØªØ§Ø²Ø©")
elif accuracy > 0.75:
    print(f"   âœ… Ø¯Ù‚Ø© Ø¬ÙŠØ¯Ø©")
elif accuracy > 0.65:
    print(f"   âš ï¸ Ø¯Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø©")
else:
    print(f"   âŒ Ø¯Ù‚Ø© Ø¶Ø¹ÙŠÙØ© - Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†")

print(f"\n3ï¸âƒ£ Data Quality Assessment:")
if df['Purchases'].std() < 0.1 and df['Visits'].std() < 0.1:
    print(f"   âš ï¸ ØªÙ†ÙˆØ¹ Ù…Ù†Ø®ÙØ¶ Ø¬Ø¯Ø§Ù‹ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù…Ø¹Ø¸Ù… Ø§Ù„Ù‚ÙŠÙ… Ù…ØªØ´Ø§Ø¨Ù‡Ø©)")
    print(f"   ğŸ’¡ ØªÙˆØµÙŠØ©: Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªÙ†ÙˆØ¹")
elif df['Purchases'].std() < 1.0 and df['Visits'].std() < 1.0:
    print(f"   âš ï¸ ØªÙ†ÙˆØ¹ Ù…Ù†Ø®ÙØ¶ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    print(f"   ğŸ’¡ ØªÙˆØµÙŠØ©: Ù‚Ø¯ ÙŠÙÙŠØ¯ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
else:
    print(f"   âœ… ØªÙ†ÙˆØ¹ Ø¬ÙŠØ¯ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")

print("\n" + "="*80)
print("âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±")
print("="*80)

input("\n\nØ§Ø¶ØºØ· Enter Ù„Ù„Ø®Ø±ÙˆØ¬...")